FROM apache/airflow:2.8.0

# Switch to root to install system packages
USER root

# Install system packages required by PySpark:
# - procps: provides 'ps' command
# - openjdk-17-jre-headless: Java runtime for Spark (matches tabulario/spark-iceberg)
RUN apt-get update -qq && \
    apt-get install -y --no-install-recommends \
    procps \
    openjdk-17-jre-headless \
    && rm -rf /var/lib/apt/lists/*

# Set JAVA_HOME environment variable
# Note: Path is architecture-specific (arm64/amd64), but the symlink handles it
ENV JAVA_HOME=/usr/lib/jvm/java-17-openjdk-arm64
ENV PATH="${JAVA_HOME}/bin:${PATH}"

# Switch back to airflow user for security
USER airflow

# Copy and install Python requirements at build time for faster container startup
COPY --chown=airflow:root processor/requirements.txt /tmp/requirements.txt
RUN pip install --no-cache-dir -r /tmp/requirements.txt && rm /tmp/requirements.txt

