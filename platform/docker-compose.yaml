services:
  minio:
    image: minio/minio
    container_name: minio
    command: server --console-address ":9001" /data
    environment:
      MINIO_ROOT_USER: admin
      MINIO_ROOT_PASSWORD: password
      MINIO_DOMAIN: minio
    ports:
      - 9000:9000 # API
      - 9001:9001 # Console
    volumes:
      - ../../../data/minio:/data # Mount local ../minio directory to /data
    networks:
      - spark-network

  postgres:
    image: postgres:16
    container_name: postgres
    ports:
      - 5432:5432
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      POSTGRES_DB: postgres # Default DB; init script creates others
    volumes:
      - ../../../data/postgres:/var/lib/postgresql/data
      - ./postgres/init-db.sh:/docker-entrypoint-initdb.d/init-db.sh
    networks:
      - spark-network

  jupyter:
    build:
      context: ..
      dockerfile: platform/Dockerfile.jupyter
    container_name: jupyter
    depends_on:
      - minio
      - postgres
    environment:
      MINIO_ROOT_USER: admin
      MINIO_ROOT_PASSWORD: password
      AWS_ACCESS_KEY_ID: admin
      AWS_SECRET_ACCESS_KEY: password
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      POSTGRES_DB: iceberg # Iceberg catalog DB
      SPARK_CONF_spark_sql_catalog_iceberg: org.apache.iceberg.spark.SparkCatalog
      SPARK_CONF_spark_sql_catalog_iceberg_type: rest
      SPARK_CONF_spark_sql_catalog_iceberg_uri: http://iceberg-rest:8181
      SPARK_CONF_spark_sql_catalog_iceberg_warehouse: s3://lakehouse/
      SPARK_CONF_spark_sql_catalog_iceberg_io__impl: org.apache.iceberg.aws.s3.S3FileIO
      SPARK_CONF_spark_sql_catalog_iceberg_s3_endpoint: http://minio:9000
    volumes:
      - ../notebooks:/home/iceberg/notebooks # for Jupyter work
      - ../processor:/home/iceberg/processor
      - ./libs/hadoop-aws-3.3.4.jar:/opt/spark/jars/hadoop-aws-3.3.4.jar
      - ./libs/aws-java-sdk-bundle-1.12.262.jar:/opt/spark/jars/aws-java-sdk-bundle-1.12.262.jar
      - ./libs/postgresql-42.7.8.jar:/opt/spark/jars/postgresql-42.7.8.jar
      - ./libs/trino-jdbc-368.jar:/opt/spark/jars/trino-jdbc-368.jar
      - ./conf/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf
      - ../../../data/lakehouse:/home/iceberg/data # Local CSV files for Jupyter work
    ports:
      - 8888:8888 # JupyterLab -> localhost:8888
    networks:
      - spark-network

  rest:
    image: apache/iceberg-rest-fixture
    container_name: iceberg-rest
    environment:
      AWS_ACCESS_KEY_ID: admin
      AWS_SECRET_ACCESS_KEY: password
      CATALOG_WAREHOUSE: s3://lakehouse/
      CATALOG_IO__IMPL: org.apache.iceberg.aws.s3.S3FileIO
      CATALOG_S3_ENDPOINT: http://minio:9000
      CATALOG_S3_PATH_STYLE_ACCESS: "true"
    ports:
      - 8181:8181
    networks:
      - spark-network
    depends_on:
    - minio
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8181/v1"]
      interval: 30s
      timeout: 10s
      retries: 5

  airflow:
    build:
      context: ..
      dockerfile: platform/Dockerfile.airflow
    container_name: airflow
    restart: always
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://postgres:postgres@postgres:5432/airflow
      AIRFLOW__CORE__FERNET_KEY: 'gVqOwBQL2OeREGfnmwnuhR5QliF0yRHcXYr5x7cyxPU='
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
    ports:
      - 8080:8080
    depends_on:
      postgres:
        condition: service_started
    volumes:
      - ./libs:/opt/airflow/libs
      - ./logs/airflow:/opt/airflow/logs
      - ../pipelines:/opt/airflow/dags # DAG files (Python DAG definitions)
      - ../processor:/opt/airflow/jobs # The container that actually runs spark-submit needs access to the .py file
      - ../models:/opt/airflow/models
      - ../../../data/lakehouse:/opt/airflow/data # Local CSV files
    networks:
      - spark-network
    command: |
      bash -c "
      airflow db init
      airflow users create --username admin --firstname Admin --lastname User --role Admin --email admin@example.com --password password || true
      airflow webserver --port 8080 &
      airflow scheduler
      "

networks:
  spark-network:
    driver: bridge
