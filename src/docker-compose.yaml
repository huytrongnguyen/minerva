services:
  minio:
    image: minio/minio
    container_name: minio
    command: server --console-address ":9001" /data
    environment:
      MINIO_ROOT_USER: admin
      MINIO_ROOT_PASSWORD: password
      MINIO_DOMAIN: minio
    ports:
      - 9000:9000 # API
      - 9001:9001 # Console
    volumes:
      - ../../../minio:/data # Mount local ../minio directory to /data
    networks:
      - spark-network

  postgres:
    image: postgres:16
    container_name: postgres
    ports:
      - 5432:5432
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      POSTGRES_DB: postgres # Default DB; init script creates others
    volumes:
      - ../../../postgres:/var/lib/postgresql/data
      - ./init-db.sh:/docker-entrypoint-initdb.d/init-db.sh
    networks:
      - spark-network

  spark-master:
    image: tabulario/spark-iceberg
    container_name: spark-master
    depends_on:
      - minio
      - postgres
    environment:
      MINIO_ROOT_USER: admin
      MINIO_ROOT_PASSWORD: password
      AWS_ACCESS_KEY_ID: admin
      AWS_SECRET_ACCESS_KEY: password
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      POSTGRES_DB: iceberg # Iceberg catalog DB
      SPARK_CONF_spark_sql_catalog_iceberg: org.apache.iceberg.spark.SparkCatalog
      SPARK_CONF_spark_sql_catalog_iceberg_type: rest
      SPARK_CONF_spark_sql_catalog_iceberg_uri: http://iceberg-rest:8181
      SPARK_CONF_spark_sql_catalog_iceberg_warehouse: s3://lakehouse/
      SPARK_CONF_spark_sql_catalog_iceberg_io__impl: org.apache.iceberg.aws.s3.S3FileIO
      SPARK_CONF_spark_sql_catalog_iceberg_s3_endpoint: http://minio:9000
    volumes:
      - ../notebooks:/home/iceberg/notebooks # for Jupyter work
      - ./processor:/home/iceberg/processor
      - ../../../data:/home/iceberg/data # Local CSV files for Jupyter work
    ports:
      - 8888:8888 # JupyterLab → localhost:8888
      - 7077:7077 # Spark Master RPC (needed for submission)
      - 8585:8080 # Spark Master Web UI → localhost:8585
    networks:
      - spark-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 90s # Give time for Spark + Jupyter to start

  spark-worker-1:
    image: tabulario/spark-iceberg
    container_name: spark-worker-1
    depends_on:
      spark-master:
        condition: service_healthy
    environment:
      SPARK_MODE: worker
      SPARK_MASTER_URL: spark://spark-master:7077
      SPARK_WORKER_CORES: 2
      SPARK_WORKER_MEMORY: 2G
      SPARK_RPC_AUTHENTICATION_ENABLED: no
      SPARK_RPC_ENCRYPTION_ENABLED: no
      SPARK_SSL_ENABLED: no
    networks:
      - spark-network
    entrypoint: /bin/bash
    command: -c "/opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077 --cores 2 --memory 2G --port 7078"

  spark-worker-2:
    image: tabulario/spark-iceberg
    container_name: spark-worker-2
    depends_on:
      spark-master:
        condition: service_healthy
    environment:
      SPARK_MODE: worker
      SPARK_MASTER_URL: spark://spark-master:7077
      SPARK_WORKER_CORES: 2
      SPARK_WORKER_MEMORY: 2G
      SPARK_RPC_AUTHENTICATION_ENABLED: no
      SPARK_RPC_ENCRYPTION_ENABLED: no
      SPARK_SSL_ENABLED: no
    networks:
      - spark-network
    entrypoint: /bin/bash
    command: -c "/opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077 --cores 2 --memory 2G --port 7079"

  rest:
    image: apache/iceberg-rest-fixture
    container_name: iceberg-rest
    environment:
      AWS_ACCESS_KEY_ID: admin
      AWS_SECRET_ACCESS_KEY: password
      AWS_REGION: us-east-1
      CATALOG_WAREHOUSE: s3://lakehouse/
      CATALOG_IO__IMPL: org.apache.iceberg.aws.s3.S3FileIO
      CATALOG_S3_ENDPOINT: http://minio:9000
      CATALOG_S3_PATH_STYLE_ACCESS: "true"
    ports:
      - 8181:8181
    networks:
      spark-network:
    depends_on:
    - minio
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8181/v1"]
      interval: 30s
      timeout: 10s
      retries: 5

  airflow:
    build:
      context: ..
      dockerfile: src/Dockerfile.airflow
    container_name: airflow
    restart: always
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://postgres:postgres@postgres:5432/airflow
      AIRFLOW__CORE__FERNET_KEY: 'gVqOwBQL2OeREGfnmwnuhR5QliF0yRHcXYr5x7cyxPU='
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
    ports:
      - 8080:8080
    depends_on:
      postgres:
        condition: service_started
      spark-master:
        condition: service_healthy
      spark-worker-1:
        condition: service_started
      spark-worker-2:
        condition: service_started
    volumes:
      - ../airflow/dags:/opt/airflow/dags # DAG files (Python DAG definitions)
      - ../airflow/jobs:/opt/airflow/jobs
      - ../airflow/logs:/opt/airflow/logs
      - ../settings:/opt/airflow/settings
      - ../libs:/opt/airflow/libs
      - ./processor:/opt/airflow/processor # The container that actually runs spark-submit needs access to the .py file
      - ../../../data:/opt/airflow/data # Local CSV files
    networks:
      - spark-network
    command: |
      bash -c "
      airflow db init
      airflow users create --username admin --firstname Admin --lastname User --role Admin --email admin@example.com --password password || true
      airflow webserver --port 8080 &
      airflow scheduler
      "

networks:
  spark-network:
    driver: bridge
